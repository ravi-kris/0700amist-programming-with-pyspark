{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "971bfe76-09f6-4ea9-974f-18c94fd878f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/07 19:57:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "   # Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"MyPySparkApp\").config(\"spark.some.config.option\", \"config-value\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2119814a-fce6-45be-9a8d-7c8566431f8c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/ravikris/Desktop/aws_py/GitHub/0700amist-programming-with-pyspark/emp.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m empDF \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43memp.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/ravikris/Desktop/aws_py/GitHub/0700amist-programming-with-pyspark/emp.csv."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e225ea-0cdd-4fe6-91b2-e09c79ac7e20",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'SparkSession' has no attribute 'getOrCreate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      3\u001b[0m    \u001b[38;5;66;03m# Create a SparkSession\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'SparkSession' has no attribute 'getOrCreate'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "   # Create a SparkSession\n",
    "spark = SparkSession.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "603c6726-3551-4229-8967-c419b9a0d1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ravis-mbp.frontiernet.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyPySparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a69e8d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94f8e83c-638d-413c-a604-4dac86a30b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF = spark.read.csv('data/emp.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57c09a31-e7e2-455a-a70d-5006b5c13f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "|emp_id|  emp_name|  emp_designation|emp_salary|emp_department|emp_join_date| emp_location|\n",
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "|   101|John Smith|Software Engineer|     75000|            IT|   15-01-2022|     New York|\n",
      "|   102|  Jane Doe|     Data Analyst|     60000|     Analytics|   20-08-2021|San Francisco|\n",
      "|   103|Mike Brown|  Product Manager|     90000|       Product|   10-05-2023|       London|\n",
      "|   104|Lisa Green|       HR Manager|     85000|            HR|   05-11-2020|         NULL|\n",
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15bca348-77e8-4da0-be7f-2b316229de0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "|emp_id|  emp_name|  emp_designation|emp_salary|emp_department|emp_join_date| emp_location|\n",
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "|   101|John Smith|Software Engineer|     75000|            IT|   15-01-2022|     New York|\n",
      "|   102|  Jane Doe|     Data Analyst|     60000|     Analytics|   20-08-2021|San Francisco|\n",
      "|   103|Mike Brown|  Product Manager|     90000|       Product|   10-05-2023|       London|\n",
      "|   104|Lisa Green|       HR Manager|     85000|            HR|   05-11-2020|         NULL|\n",
      "+------+----------+-----------------+----------+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2807fcd7-8dd1-4c60-b333-b435ec1a725a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_id', 'string'),\n",
       " ('emp_name', 'string'),\n",
       " ('emp_designation', 'string'),\n",
       " ('emp_salary', 'string'),\n",
       " ('emp_department', 'string'),\n",
       " ('emp_join_date', 'string'),\n",
       " ('emp_location', 'string')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d48fa2d-cfe1-4620-9b62-d0773fb13bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "empDF = spark.read.csv('data/emp.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b400fa1-32f7-4e09-9fd6-e3345640246a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('emp_id', 'int'),\n",
       " ('emp_name', 'string'),\n",
       " ('emp_designation', 'string'),\n",
       " ('emp_salary', 'int'),\n",
       " ('emp_department', 'string'),\n",
       " ('emp_join_date', 'string'),\n",
       " ('emp_location', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f80382fb-b3d9-4683-95dd-7515d4ccb145",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedcolumnsDF = empDF['emp_name', 'emp_salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3706732c-e94f-43f8-b0af-076ef610b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|  emp_name|emp_salary|\n",
      "+----------+----------+\n",
      "|John Smith|     75000|\n",
      "|  Jane Doe|     60000|\n",
      "|Mike Brown|     90000|\n",
      "|Lisa Green|     85000|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selectedcolumnsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10a1d384-dac9-46b2-bbf9-439c3d8d38ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+-----------------+----------+--------------+-------------+------------+\n",
      "|emp_id|  emp_name|  emp_designation|emp_salary|emp_department|emp_join_date|emp_location|\n",
      "+------+----------+-----------------+----------+--------------+-------------+------------+\n",
      "|   101|John Smith|Software Engineer|     75000|            IT|   15-01-2022|    New York|\n",
      "|   103|Mike Brown|  Product Manager|     90000|       Product|   10-05-2023|      London|\n",
      "|   104|Lisa Green|       HR Manager|     85000|            HR|   05-11-2020|        NULL|\n",
      "+------+----------+-----------------+----------+--------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empDF.filter(empDF.emp_salary> 70000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee62672d-43f0-464f-ac3d-8d61a8090750",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filteredDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfilteredDF\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filteredDF' is not defined"
     ]
    }
   ],
   "source": [
    "filteredDF = empDF.emp_salary> 70000).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff3f7f-cd3c-4f6b-b823-ec5e0e5d2e49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
